{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from env import Hangman, HangmanCheat\n",
    "from network import NNAgent, Network, CheatAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the orignal policy\n",
    "policy_network = Network()\n",
    "policy_network.load_weights('models/policy.h5') #input p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Honest* case\n",
    "\n",
    "Analyze the given policy in the case, where the environment (so the second player) is honest.\n",
    "\n",
    "Derive how many words are guessed correctly using the top 1000 frequent words for different maximum number of guesses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# results dictionary for max_lives in range 1-10\n",
    "results = {}\n",
    "\n",
    "# original agent\n",
    "player = NNAgent(policy_network)\n",
    "player.eval()\n",
    "\n",
    "for max_lives in range(1, 11):\n",
    "    print(f\"Testing with max_lives = {max_lives}\")\n",
    "    print(\"=====================================\")\n",
    "\n",
    "    # (re-)initialize the environment\n",
    "    env = Hangman('words/top_1000.txt' , max_lives = max_lives, verbose = False)\n",
    "    player.reset_guessed()\n",
    "\n",
    "    # count the number of correct guesses\n",
    "    correct = 0\n",
    "\n",
    "    # loop through all the words\n",
    "    for word in tqdm(env.words):\n",
    "        # reset the environment\n",
    "        state = env.reset(word)\n",
    "        done = False\n",
    "        # play the game\n",
    "        while not done:\n",
    "            guess = player.select_action(state)\n",
    "            state, _ , done , _ = env.step(guess)\n",
    "        # check if the word is guessed correctly\n",
    "        if env.is_game_won():\n",
    "            correct += 1\n",
    "        player.reset_guessed()\n",
    "    results[max_lives] = correct / len(env.words)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-Tuned Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# results dictionary for max_lives in range 1-10\n",
    "results = {}\n",
    "\n",
    "# fine-tuned agent\n",
    "player = CheatAgent(policy_network)\n",
    "\n",
    "for max_lives in range(1, 11):\n",
    "    print(f\"Testing with max_lives = {max_lives}\")\n",
    "    print(\"=====================================\")\n",
    "\n",
    "    # (re-)initialize the environment\n",
    "    env = Hangman('words/top_1000.txt' , max_lives = max_lives, verbose = False)\n",
    "    player.reset_guessed()\n",
    "\n",
    "    # count the number of correct guesses\n",
    "    correct = 0\n",
    "\n",
    "    # loop through all the words\n",
    "    for word in tqdm(env.words):\n",
    "        # reset the environment\n",
    "        state = env.reset(word)\n",
    "        done = False\n",
    "        # play the game\n",
    "        while not done:\n",
    "            guess = player.select_action(state)\n",
    "            state, _ , done , _ = env.step(guess)\n",
    "        # check if the word is guessed correctly\n",
    "        if env.is_game_won():\n",
    "            correct += 1\n",
    "        player.reset_guessed()\n",
    "    results[max_lives] = correct / len(env.words)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Cheat* case\n",
    "\n",
    "\n",
    "### Original Policy\n",
    "Analyze the given policy in the case, where the environment (so the second player) is cheating.\n",
    "\n",
    "Keep the number of maximum guesses fixed at first. Derive how many words are guessed correctly using the top 1000 frequent words and vary the cheating factor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# results dictionary for reject_rate in range 0-1 and max_lives in range 1-10\n",
    "results = {}\n",
    "\n",
    "# original agent\n",
    "player = NNAgent(policy_network)\n",
    "player.eval()\n",
    "\n",
    "# reject rate\n",
    "reject_rate = 0.1\n",
    "\n",
    "for max_lives in range(1, 11):\n",
    "    print(f\"Testing with max_lives = {max_lives}\")\n",
    "    print(\"=====================================\")\n",
    "\n",
    "    # (re-)initialize the environment\n",
    "    env = HangmanCheat('words/sample_1000.txt' , max_lives = max_lives, verbose = False, reject_rate=reject_rate)\n",
    "    player.reset_guessed()\n",
    "\n",
    "    # count the number of correct guesses\n",
    "    correct = 0\n",
    "\n",
    "    # loop through all the words\n",
    "    for word in tqdm(env.words):\n",
    "        # reset the environment\n",
    "        state = env.reset(word)\n",
    "        done = False\n",
    "        # play the game\n",
    "        while not done:\n",
    "            guess = player.select_action(state)\n",
    "            state, _ , done , _ = env.step(guess)\n",
    "        # check if the word is guessed correctly\n",
    "        if env.is_game_won():\n",
    "            correct += 1\n",
    "        player.reset_guessed()\n",
    "    results[(max_lives, reject_rate)] = correct / len(env.words)\n",
    "\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-Tuned Policy\n",
    "\n",
    "Fine-tune the policy to the cheating environment. Derive how many words are guessed correctly using the top 1000 frequent words and vary the cheating factor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print a loss curve for the policy at different epochs\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "cheat_player = CheatAgent(Network())\n",
    "\n",
    "results = {\n",
    "    'epoch': [],\n",
    "    'loss': []\n",
    "}\n",
    "\n",
    "for ep in tqdm(range(0, 20001, 1000)):\n",
    "    cheat_player.load_weights(f'models/finetuned/policy_finetuned_{ep}.h5')\n",
    "    cheat_player.reset_guessed()\n",
    "    cheat_env = HangmanCheat('words/top_1000.txt' , max_lives = 6, verbose = False)\n",
    "\n",
    "    # take an average loss of 20 random sampled words\n",
    "    losses = []\n",
    "    for _ in range(20):\n",
    "        state = cheat_env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            guess = cheat_player.select_action(state)\n",
    "            state, reward, done, ans = cheat_env.step(guess)\n",
    "        cheat_player.finalize_episode(ans)\n",
    "        loss = cheat_player.train_model()\n",
    "        losses.append(loss)\n",
    "    \n",
    "    results['epoch'].append(ep)\n",
    "    results['loss'].append(np.mean(losses))\n",
    "\n",
    "# save the results\n",
    "import pandas as pd\n",
    "loss_df = pd.DataFrame(results)\n",
    "loss_df.to_csv('results/finetuning_loss.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(results['epoch'], results['loss'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Finetuning Loss')\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/finetuning_loss.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# results dictionary for reject_rate in range 0-1 and max_lives in range 1-10\n",
    "results = {\n",
    "    'max_lives': [],\n",
    "    'reject_rate': [],\n",
    "    'win_rate': [],\n",
    "    'num_words_cheated': [],\n",
    "    'num_words_doubted': [],\n",
    "    'num_words_doubted_correct': []\n",
    "}\n",
    "\n",
    "# fine-tuned agent\n",
    "cheat_player = CheatAgent(Network())\n",
    "cheat_player.load_weights('models/finetuned/policy_finetuned_3000.h5')\n",
    "cheat_player.eval()\n",
    "\n",
    "# reject rate\n",
    "reject_rate = 0.1\n",
    "\n",
    "for max_lives in range(9, 11):\n",
    "    print(f\"Testing with max_lives = {max_lives}\")\n",
    "    print(\"=====================================\")\n",
    "\n",
    "    # (re-)initialize the environment\n",
    "    env = HangmanCheat('words/top_1000.txt' , max_lives = max_lives, verbose = False, reject_rate=reject_rate)\n",
    "    cheat_player.reset_guessed()\n",
    "\n",
    "    # count the number of correct guesses\n",
    "    wins = 0\n",
    "    num_words_cheated = 0\n",
    "    num_words_doubted = 0\n",
    "    num_words_doubted_correct = 0\n",
    "\n",
    "    # loop through all the words\n",
    "    for word in tqdm(env.words):\n",
    "        # reset the environment\n",
    "        state = env.reset(word)\n",
    "        done = False\n",
    "        # play the game\n",
    "        while not done:\n",
    "            guess = cheat_player.select_action(state)\n",
    "            state, _ , done , ans = env.step(guess)\n",
    "        if env.is_game_won():\n",
    "            wins += 1\n",
    "        if ans['cheated'] == True:\n",
    "            num_words_cheated += 1\n",
    "        if ans['doubted'] == True:\n",
    "            num_words_doubted += 1\n",
    "        if ans['doubted'] == True and ans['cheated'] == True and (ans['doubted_step'] > ans['cheated_step']):\n",
    "            num_words_doubted_correct += 1\n",
    "        cheat_player.reset_guessed()\n",
    "\n",
    "    # append results to the dictionary\n",
    "    results['max_lives'].append(max_lives)\n",
    "    results['reject_rate'].append(reject_rate)\n",
    "    results['win_rate'].append(wins / len(env.words))\n",
    "    results['num_words_cheated'].append(num_words_cheated)\n",
    "    results['num_words_doubted'].append(num_words_doubted)\n",
    "    results['num_words_doubted_correct'].append(num_words_doubted_correct)\n",
    "\n",
    "    print(f\"({max_lives}, {reject_rate}): Win rate {wins / len(env.words)}, Num words cheated {num_words_cheated}, Num words doubted {num_words_doubted}, Num words doubted correct {num_words_doubted_correct}\")\n",
    "\n",
    "    # save the results\n",
    "    import pandas as pd\n",
    "    df = pd.DataFrame(results)\n",
    "    df.to_csv('results/cheat_agent_top_1000_ml9-11.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top 1000 words, Original Policy\n",
    "\n",
    "| max_lives | 0.0   | 0.1   | 0.2   | 0.3   | 0.4   | 0.5   | 0.6   | 0.7   | 0.8   | 0.9   | 1.0   |\n",
    "|----------:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|\n",
    "| 1         | 0.013 | 0.005 | 0.005 | 0.002 | 0.001 | 0.000 | 0.000 | 0.000 | 0.000 | 0.000 | 0.000 |\n",
    "| 2         | 0.051 | 0.030 | 0.020 | 0.012 | 0.011 | 0.004 | 0.001 | 0.000 | 0.000 | 0.000 | 0.000 |\n",
    "| 3         | 0.106 | 0.065 | 0.033 | 0.018 | 0.005 | 0.001 | 0.000 | 0.003 | 0.002 | 0.001 | 0.000 |\n",
    "| 4         | 0.210 | 0.127 | 0.070 | 0.040 | 0.019 | 0.013 | 0.006 | 0.003 | 0.000 | 0.000 | 0.000 |\n",
    "| 5         | 0.317 | 0.178 | 0.107 | 0.061 | 0.033 | 0.014 | 0.010 | 0.004 | 0.000 | 0.000 | 0.000 |\n",
    "| 6         | 0.434 | 0.251 | 0.155 | 0.069 | 0.046 | 0.024 | 0.009 | 0.006 | 0.003 | 0.001 | 0.000 |\n",
    "| 7         | 0.552 | 0.307 | 0.176 | 0.108 | 0.040 | 0.025 | 0.018 | 0.004 | 0.006 | 0.002 | 0.000 |\n",
    "| 8         | 0.646 | 0.375 | 0.193 | 0.128 | 0.066 | 0.038 | 0.014 | 0.006 | 0.002 | 0.002 | 0.000 |\n",
    "| 9         | 0.721 | 0.432 | 0.268 | 0.148 | 0.074 | 0.033 | 0.017 | 0.006 | 0.005 | 0.001 | 0.000 |\n",
    "| 10        | 0.781 | 0.458 | 0.274 | 0.150 | 0.082 | 0.046 | 0.017 | 0.014 | 0.002 | 0.003 | 0.000 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample 1000 words, Original Policy\n",
    "\n",
    "| max_lives | 0.0   | 0.1   | 0.2   | 0.3   | 0.4   | 0.5   | 0.6   | 0.7   | 0.8   | 0.9   | 1.0   |\n",
    "|----------:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|\n",
    "| 1         | 0.020 | 0.008 | 0.002 | 0.000 | 0.001 | 0.000 | 0.000 | 0.000 | 0.000 | 0.000 | 0.000 |\n",
    "| 2         | 0.067 | 0.030 | 0.015 | 0.006 | 0.002 | 0.001 | 0.000 | 0.000 | 0.001 | 0.000 | 0.000 |\n",
    "| 3         | 0.148 | 0.079 | 0.032 | 0.022 | 0.004 | 0.004 | 0.000 | 0.000 | 0.000 | 0.000 | 0.000 |\n",
    "| 4         | 0.239 | 0.113 | 0.057 | 0.022 | 0.009 | 0.005 | 0.000 | 0.002 | 0.000 | 0.000 | 0.000 |\n",
    "| 5         | 0.360 | 0.188 | 0.090 | 0.051 | 0.018 | 0.008 | 0.001 | 0.002 | 0.001 | 0.000 | 0.000 |\n",
    "| 6         | 0.493 | 0.268 | 0.128 | 0.066 | 0.025 | 0.015 | 0.007 | 0.003 | 0.000 | 0.000 | 0.000 |\n",
    "| 7         | 0.581 | 0.291 | 0.152 | 0.066 | 0.045 | 0.017 | 0.006 | 0.001 | 0.000 | 0.000 | 0.000 |\n",
    "| 8         | 0.659 | 0.338 | 0.197 | 0.080 | 0.035 | 0.017 | 0.006 | 0.003 | 0.000 | 0.000 | 0.000 |\n",
    "| 9         | 0.730 | 0.377 | 0.200 | 0.099 | 0.043 | 0.020 | 0.005 | 0.001 | 0.000 | 0.001 | 0.000 |\n",
    "| 10        | 0.777 | 0.414 | 0.226 | 0.094 | 0.051 | 0.025 | 0.009 | 0.005 | 0.000 | 0.001 | 0.000 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 1000 words, Fine-Tuned Policy\n",
    "\n",
    "| max_lives | 0.0   | 0.1   | 0.2   | 0.3   | 0.4   | 0.5   | 0.6   | 0.7   | 0.8   | 0.9   | 1.0   |\n",
    "|----------:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|\n",
    "| 1         | 0.015 | 0.014 | 0.007 | 0.005 | 0.006 | 0.008 | 0.004 | 0.002 | 0.003 | 0.001 | 0.004 |\n",
    "| 2         | 0.048 | 0.033 | 0.023 | 0.02  | 0.008 | 0.007 | 0.008 | 0.011 | 0.006 | 0.01  | 0.008 |\n",
    "| 3         | 0.112 | 0.072 | 0.048 | 0.031 | 0.019 | 0.018 | 0.017 | 0.014 | 0.01  | 0.013 | 0.009 |\n",
    "| 4         | 0.185 | 0.117 | 0.087 | 0.07  | 0.053 | 0.043 | 0.046 | 0.037 | 0.039 | 0.032 | 0.036 |\n",
    "| 5         | 0.285 | 0.195 | 0.143 | 0.112 | 0.084 | 0.079 | 0.056 | 0.065 | 0.07  | 0.077 | 0.073 |\n",
    "| 6         | 0.356 | 0.273 | 0.236 | 0.202 | 0.148 | 0.141 | 0.142 | 0.143 | 0.146 | 0.131 | 0.139 |\n",
    "| 7         | 0.417 | 0.397 | 0.343 | 0.296 | 0.304 | 0.254 | 0.265 | 0.253 | 0.234 | 0.238 | 0.223 |\n",
    "| 8         | 0.499 | 0.506 | 0.458 | 0.426 | 0.436 | 0.408 | 0.415 | 0.398 | 0.392 | 0.437 | 0.432 |\n",
    "| 9         | 0.556 | 0.574 | 0.585 | 0.584 | 0.573 | 0.605 | 0.579 | 0.594 | 0.579 | 0.586 | 0.624 |\n",
    "| 10        | 0.565 | 0.641 | 0.65  | 0.703 | 0.715 | 0.73  | 0.742 | 0.739 | 0.745 | 0.757 | 0.726 |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
